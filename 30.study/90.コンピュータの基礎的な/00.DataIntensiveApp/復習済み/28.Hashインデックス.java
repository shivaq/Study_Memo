■■■■■■■■■■■■■■■■■■■■■■■■■■ インデックス
・キーら簡単にを見つけ出すためのもの
・メタデータを付与する。
・データの検索方法が異なれば、インデックスも異なる。

▼ インデックスのコスト
・書き込み時にオーバーヘッドが生じる
・データが書き込まれるたびに、インデックスは更新されなければならない。
・普通はデフォルトでインデックスを付けない
・アプリ固有のクエリ方法に基づいたインデックスを先tなくすることで、コストを最小限にする。



■■■■■■■■■■■■■■■■■■■■■■■■■■ Hash Indexes
・key-value data のためのインデックス。
・キーバリューは辞書型に似ている。
・通常 HashMapとして実装されている

メモリ上のデータ構造でハッシュマップを持っている。
 →ディスク上のデータをインデックス化するために、ハッシュマップを使う。

例)
条件：ストレージが、consists only of appending to a file だとする。
最もシンプルなインデックス化方法：
1.メモリ内のハッシュマップを保持する。

▼ キー新規追加/更新時
1.in memory hash map に、すべてのキーと、そのデータファイル内での byte offset(そのファイルの何バイト目にそのキーが位置しているか) とをマップしたものを保持する
2.新しい キー/バリューをappendする
 →ハッシュマップを更新して、appendしたデータのオフセットを反映させる

　▼ 値検索時
1.メモリ上のハッシュマップを使ってデータファイル内での、そのキーのオフセットを見つける
2.その場所を探す
3.値を読み込む

■■■■■■■■■■■■■■■■■■■■■■■■■■ セグメントとCompaction
▼ 単純にappendするだけだとしたら、ディスク枯渇をどう回避するのだ？

▼ ログを、一定のサイズごとに分割する
 →一定のサイズに達したら、a segment file をクローズする。
  →続く書き込みを、新規セグメントファイルに対して行う。
   →それらセグメントに対し Compaction を行う。

▼ Compaction & Merging
Compaction：重複するキーをすてる
  →直近で更新したキー(の値)のみを保持する
   →セグメントがぐっと小さくなる
   ※ どのタイミングでCompaction を行うのだろう？
Merge:Compaction を行うと同時に、複数のセグメントをマージする
 →つまり、レコード更新等で、重複することになったキーの最新バージョン以外を捨てて、
 かつ、増加しつつあるセグメントをマージして、さらに重複を最小化する
※セグメントは書き込み後にmodifyされない
 →よって、マージされたセグメントは新規ファイルに書き込まれる。
※アクティブではないセグメントのマージと compaction はバックグラウンドスレッドで行われる
 →で、それが行われている間も、読み書きリクエストは古いファイルを使って引き続き行われる
  →マージ完了後、読み込みリクエストは新しくマージされたセグメントに対して行われる
   →古いセグメントファイルは削除される
    →各セグメントは、固有のハッシュテーブルを持ち、キーとオフセットをマップした状態になる。

▼ キーの値を探す時
直近のセグメントのハッシュマップを探す
 →キーがない場合
  →2番めに新しいセグメントを探す →三番目に新しいセグメントを。。。。

▼ マージプロセスはセグメントを小さく保つ。
 →検索で探す対象のハッシュマップの数も小さく保たれる。

■■■■■■■■■■■■■■■■■■■■■■■■■■ セグメントとCompaction の使われ方
▼ File format
CSVなんかより、バイナリフォーマットのほうが早くてシンプル。
the raw 文字列のあとに、文字列の長さを bytesにエンコードしてくっつける

▼ Deleting records
キーバリューを削除する時、データファイルに削除レコードをappendする(AKA tombstone)
 →log セグメントがマージされる時、tombstone からマージプロセスに、削除されるキーのすべての previous 値を削除する

▼ Crash recovery
DBが再起動された場合、メモリ内ハッシュマップはロストする。
実際は、各セグメントのハッシュマップは、セグメントファイル全体を最初から最後まで読み込むことで復旧できる。
しかし、セグメントファイルが大きいと時間がかかる。
※ Bitcask は各セグメントのハッシュマップのスナップショットをディスクに書き込むことで、メモリへのロードを早くしている

▼ Partially written records
レコード書きかけの状態でDBがクラッシュすることだってある。
Bitcast のファイルにはチェックサムがあるので、書きかけの一部がログにある場合、そいつは削除対象ということで無視する。

▼ Concurrency control
・書き込みは、ログにきっちりシークエンシャルにappendされるので、
一般的な実装は、書き込みスレッドをひとつだけにしている。
・データファイルのセグメントはイミュータブルなので、複数のスレッドから同時に読み込みすることができる。

■■■■■■■■■■■■■■■■■■■■■■■■■■ append only ログはぱっと見無駄に見える
 →ファイル更新する時に、古い値を新しい値で上書きしたらいいじゃない？

▼ append only が選ばれる理由
・append も セグメントのマージも、シークエンシャルな書き込み操作である
 →これはランダムな書き込みよりずっと早い。※特に磁気HDへの書き込みの場合 ※ SSD への書き込みでも、シークエンシャルな書き込みのほうが好まれる

・Concurrency とクラッシュリカバリは、append only または イミュータブルな方がずっと早い
 →値を上書きしている最中にクラッシュしたらどうしよう、なんて心配はいらない。

・古いセグメントをマージすることで、データファイルの断片化問題が回避できる

▼ ハッシュテーブルインデックスの限界
・ハッシュテーブルはメモリに収まらないといけない。キーが多すぎると使えない。
ディスクにハッシュマップを維持することもできるが、そうするとパフォーマンスが良くない
random access I/O が多くなるし、ハッシュ衝突も起こりうるし。。。
・範囲クエリが非効率。例えば、kitty00000 and kitty99999 といった範囲のキーをスキャンする場合、ハッシュマップ内の個々のキーを探索しなければならない。
