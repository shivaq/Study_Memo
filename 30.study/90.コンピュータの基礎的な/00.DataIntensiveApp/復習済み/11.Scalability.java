▼ Scalability
システムのボリューム、トラフィック、複雑性がました時に、合理的な方法で対応できること



■■■■■■■■■■■■■■■■■■■■■■■■■■ まず、現在の負荷を知れ
・現在の負荷を把握した上でのみ、システムが成長した時、同対処する？といった問いができる

■■■■■■■■■■■■■■■■■■■■■■■■■■ load parameters
・アーキテクチャによって、適したパラメータは異なる
例) Webサーバに対する request/sec
・DB の read/writes 比率
・チャットルームでの同時アクティブユーザー数
・キャッシュのヒットレート

■■■■■■■■■■■■■■■■■■■■■■■■■■ ツイッターの例
▼ ツイッターのサービスに有効な、スケーラビリティを考えるための負荷指標
・フォロワーへの配信量/ユーザー
※ 上記に、ユーザーあたりのツイート頻度で weight したりしてるかも


▼ ツイッターの主要な操作
・ツイートをする
・タイムライン

▼ fan-out がスケーリング上の課題となる
・ツイートの数が一番の課題ではない
・fan-out: ユーザーがそれぞれ多数の人々をフォローし、それぞれのユーザーも多くの人々からフォローされている

▼ ツイッターの主要な操作の実装方法
・方法1 を採用していたが、負荷に耐えられず方法2に移行した

▼ 方法1
・ツイートをする
 →グローバルなツイートの集合に新規ツイートを挿入する
・ユーザーがホームタイムラインをリクエストする
 →ユーザーがフォローしている人々を照会する
 →フォロー対象のすべてのツイートを探す
 →それらツイートを時間でソート
 →マージ

SELECT tweets.*, users.* FROM tweets
JOIN users ON tweets.sender_id = users.id
JOIN follows ON follows.followee_id = users.id
WHERE follows.follower_id = current_user

▼ 方法2
・各ユーザーのホームタイムラインのキャッシュを維持する ※ ツイートのメールボックスのようなイメージ
・あるユーザーがツイートする
 →そのユーザーをフォローしているすべてのユーザーを検索する
 →その新規ツイートを各ユーザーのホームタイムラインキャッシュに挿入する
結果、ホームタイムラインへのリクエストは cheap になる。なぜなら、その結果は事前に計算されているから。

▼ 方法2 の盲点
・平均で一つのツイートは75のフォロワーに配信される。
・4.6k ツイート/sec なので、ホームタイムラインへの書き込みは、 345k/sec となる。
・しかし、平均の値で考えると物事を見誤る
・ユーザーによっては、3000万フォロワーがいたりする
 →一つのツイートが、ホームタイムラインへの 3000万 書き込みを必要とすることになる
  ※なお、ツイッターはすべてのフォロワーに5秒以内にツイートを配信することを目標にしている

▼ ハイブリッドな方法
・セレブリティのユーザーは、fan-out する可能性がある
・セレブリティのツイートに関しては、その他のユーザーのツイートとは別に取得して、方法1のように、ユーザーたちのホームタイムラインにマージする

■■■■■■■■■■■■■■■■■■■■■■■■■■ 負荷が増えたときの対応
▼ 考え方1
・負荷が高まり、かつCPU等のリソースを維持する場合、システムのパフォーマンスにどんな影響がある？

▼ 考え方2
・負荷が高まった時、パフォーマンスを維持する場合、どれくらいリソースを増やせばいい？

▼ Hadoop などのバッチ処理システムでのパフォーマンスについての考え方
・スループット(処理できるプロセスの数/sec)を考慮
・あるサイズのデータセットに対するジョブにかかるトータル時間を考慮
※ 理想上ではバッチジョブの実行時間はデータセットのサイズ/スループット で求めることができる。
※ しかし実際は、skew(歪み)(データがワーカープロセスに均等に振り分けられてないこと)と一番遅いタスクを待つ必要があることから、実行時間は長くなる。

▼ オンラインシステムでのパフォーマンスについての考え方
・レスポンスタイム(クライアントがリクエストを投げて、レスポンスを受け取るまでの時間)

■■■■■■■■■■■■■■■■■■■■■■■■■■ レスポンスタイム
▼ レスポンスタイム
・クライアント視点。サービスタイム(リクエストを処理する実際の時間) ＋ ネットワークの遅延 ＋ キューの遅延

▼ レイテンシー
リクエストがハンドルのを待っている時間

▼ レスポンスタイムの捉え方
・レスポンスタイムは同じ処理を行った場合でもいつも同じなわけではない。
・レスポンスタイムを単一の数値とは考えず、計測可能な値の distribution(分布) だと捉える必要がある

▼ percentile 百分位数
・百分位数は、データセットを部分に分ける
・通常、n番目の百分位数の場合、n番目より下位の観測値の割合はn%で、n番目より上位の観測値の割合は（100 - n）%となります。
・度数分布で与えられた全データを百等分した点で、50番目の百分位数が中央値にあたる。



▼ 中央値はだめだ
・典型的なレスポンスタイムを知りたい場合、中央値は適していない。その中央値は、どれくらいのユーザーが、実際にその遅延を経験しているかはわからない。
・中央レスポンスタイムが 200ms だった場合、リクエストのうち半分は200ms 以下で戻ってきており、残り半分はそれよりも長くかかっている、ということを示しているに過ぎない
・中央値 AKA 50 番目のパーセンタイル AKA p50
・p50 は単一のリクエストのことを指している。
・ユーザーが複数回リクエストをした場合、中央値より遅いリクエストの少なくとも1つは50％よりずっと多く存在する可能性がある

▼ 異常値がどれくらいひどいかをしるために
・より大きいパーセンタイルを見る p95, p99, p99.9
もしp95 のレスポンスタイムが 1.5secだったら、100のリクエストのうち95 のリクエストが1.5sec 以下のレスポンスタイムということになる。
そして、100のうち5つは1.5sec以上ということになる。
・アマゾンはp99.9 を採用。なぜなら、レスポンスが遅くなる顧客というのは、しばしば最も多くのデータを利用している、つまりはたくさん購入してくれるこきゃくだから。
・p99.99 はexpensiveすぎて、アマゾンの利益にならない

▼ キューのディレイがレスポンスタイムの大きな範囲を占める
・下記理由により、レスポンスタイムはクライアント側で計測するべき
・サーバは少ない数の処理しか並行処理できない。CPUの数は限られているから。
一部の遅いリクエストが、続くリクエストを待たせている時間が、全体のリクエストの時間に影響を与える。(head-of-lline blocking)
続くプロセスがどんなに早くても、クライアント側では全体のレスポンスタイムがおそいのだから、遅く感じる。
・人工的に負荷をかけるテストをするときは、前のリクエストの完了とかを待たずにリクエストを送り続けなければならない。


▼ レスポンスタイム Percentile をモニタリングダッシュボードに追加する方法
・直近 10 分間の rolling-window でレスポンスタイムjを見たいとする
・1分ごとに中央値と複数のパーセンタイルを該当ウィンドウ内で計算し、そのメトリクスをグラフにプロットする
・おおよそのパーセンタイルを計算するアルゴリズムとしては下記がある
・forward decay, t-digest, HdrHistogram

▼ 無意味な計算
パーセンタイルの平均値、複数のマシンのデータを結合する。
▼ 正しいレスポンスタイムの統合方法
ヒストグラムを追加する

▼ スケールアップかスケールアウト化
単一のマシンで動くシステムは、よりシンプルだ。しかし、ハイエンドのマシンは高くつく。
よって、集中するワークロードに対処するには、スケールアウトは避けられない。

▼ スケールアウトとアップのミックス
複数の強力なマシンを使うことは、小さなVMを多数運用するよりも、シンプルで安価になりうる

▼ 自動スケールと手動スケール
・Elastic なシステムは、負荷の予測が困難な場合に有効。しかし、手動でスケールするほうが、シンプルでオペレーション上の予想外の事象が起こりづらい
・ステートレスなサービスを複数のマシンに分散させるのは合理的なやり方に見える一方、ステートフルなデータを分散させるのはたくさんの複雑さを混入させる。
よって、最近まで、スケールアップのコストや高可用性の要件がそれを強いることがない限り、データベースはシングルノードでスケールアップするのがよいとされていた。
・将来的に分散データシステムがデフォルトとなるのはイメージしやすい。大量のデータやトラフィックをハンドルしないユースケースでさえも。

■■■■■■■■■■■■■■■■■■■■■■■■■■ スケーラブルなシステムのためのアーキテクチャ？
・あらゆるアプリに適応可能なスケーラブルシステムなんてものは存在しない。
課題となること
・read のボリューム
・write のボリューム
・格納するデータのボリューム
・データの複雑さ
・レスポンスタイム要件
・アクセスパターン
想像してみてよ
各リクエストが１KBのサイズの100000リクエスト/sec を操作するためのシステムデザインと、
3/sec で各リクエストが2GBのサイズのシステムとでは、大きく異なるデザインになる

・アーリーステージのスタートアップや、うまくいくかどうかわからない製品は、将来訪れると予想している仮の負荷に耐えれるようスケールすることよりも、
トライアンドエラーをすばやく回すことがより重要。
