#### Cloud Computing and Supercomputing

#### large-scale computing systems: を構築する上での哲学スペクトラム
### スケールしたシステムの極点 HPC
* high-performance computing (HPC)のぶんや。
* Super computers with thousands of CPUs are typically used for computationally intensive scientific computing tasks,
* such as weather forecasting or
 molecular dynamics (simulating the movement of atoms and molecules).

### もう一つの極点 cloud computing
* multi-tenant datacenters
* commodity computers connected with an IP network (often Ethernet)
* elastic/on-demand resource allocation, and metered billing.
* Traditional enterprise datacenters lie somewhere between these extremes.

## supercomputer の fault 対応
* a job typically checkpoints the state of its computation
* to durable storage from time to time.

#### If one node fails, a common solution
* simply stop the entire cluster workload.
* After the faulty node is repaired, the computation is restarted from the last checkpoint
* Thus, a supercomputer is more like a single-node computer than a distributed system:
* it deals with **partial failure** by letting it escalate into **total failure**
* if any part of the system fails, just let everything crash (like a kernel panic on a single machine).
* In this book we focus on systems for implementing internet services, which usually look very different from supercomputers:

## internet services が fault をどう扱うか
### Many internet-related applications are online
* they need to be able to serve users with low latency at any time.
* Making the service unavailable— for example, stopping the cluster for repair—is not acceptable.
* In contrast, offline (batch) jobs like weather simulations can be stopped and restarted with fairly low impact.

### Supercomputers are typically built from specialized hardware
* where each node is quite reliable
* nodes communicate through shared memory and remote direct memory access (RDMA).

### nodes in cloud services
* On the other hand, nodes in cloud services are built from commodity machines
* which can provide equivalent performance at lower cost due to economies of scale,
* but also have higher failure rates.

### Large datacenter networks are often based on IP and Ethernet
* arranged in Clos topologies to provide high bisection bandwidth [9].

### Supercomputers often use specialized network topologies
* such as multi-dimensional meshes and toruses ,
* which yield better performance for HPC workloads with known communication patterns.

### The bigger a system gets, the more likely it is that one of its components is broken.
* Over time, broken things get fixed and new things break
* but in a system with thousands of nodes, it is reasonable to assume that something is always broken

### 常に何かが壊れてるなら、壊れててもやっていける設計にしないといけない
* When the error handling strategy consists of simply giving up
* a large system can end up spending a lot of its time recovering from faults rather than doing useful work.

### If the system can tolerate failed nodes and still keep working as a whole
* that is a very useful feature for operations and maintenance
* for example, you can perform a rolling upgrade
* restarting one node at a time, while the service continues serving users without interruption.
* In cloud environments, if one virtual machine is not performing well, you can just kill it and request a new one (hoping that the new one will be faster).
* In a geographically distributed deployment (keeping data geographically close to your users to reduce access latency), communication most likely goes over the internet, which is slow and unreliable compared to local networks.

* Supercomputers generally assume that all of their nodes are close together.


# 分散システムを扱うならば、部分失敗を許容して、fault-tolerance mechanismsをSWに組み込むべし
### we need to build a reliable system from unreliable components
* ノード数の少ない小さなシステムも同様。
* 数が少なきゃ安定するが、いつか不安定になるし、それをSW側で対処しなくちゃいかん

## The fault handling must be part of the software design,
 and you (as operator of the software) need to know what behavior to expect from the software in the case of a fault.

## It is important to consider a wide range of possible faults
* even fairly unlikely ones—and to artificially create such situations in your testing environment
to see what happens.

## In distributed systems, suspicion, pessimism, and paranoia pay off.
