■■■■■■■■■■■■■■■■■■■■■■■■■■ Partitioning of Key-Value Data
巨大なデータが有り、それをパーティションしたいとする。
それでは、
どのレコードをどのノードに格納するか、どうやって決める？

なぜパーティショニングを行うか。
ゴールは
データとクエリの負荷をノード間に均等に拡散させることだ。
もし、各ノードに均等に負荷分散されれば、
理論上は
10個のノードは、一つのノードより
10倍のデータを処理できて、読み書きスループットも10倍になるはずだ。// レプリケーションについてはここでは無視する

▼ skewed：歪んだ
パーティショニング がフェアではない場合、
あるパーティションが他のパーティションより多くのデータやクエリを抱える場合、
それを skewed と呼ぶ

▼ hot spot
skew がある状態では、パーティショニングは非効率になる。
あるパーティションに著しい高負荷が集中しているとき、それを hot spot と呼ぶ。

▼ シンプルだが欠点のあるアプローチ
ホットスポット回避のシンプルな方法は、レコードをノードにランダムに割り当てることだ。
そうすれば、データが均等にノード間に分配される。
だがしかし、
アイテムを読み込もうとした時に、ｌ
そのアイテムがどのノードにあるか、といった手がかりがないため、
全てのノードにパラレルでクエリしなければならない。

▼ よりよい方法
シンプルなキーバリューデータモデルがあるとする。
そしてプライマリキーによってそいつにアクセスできるとする。
百科事典では、
エントリーをタイトルから探すことができるでしょう？
それらはみなアルファベティカルにそーとされているから。
目当てのデータを探すのも簡単。

■■■■■■■■■■■■■■■■■■■■■■■■■■ Partitioning by Key Range
▼ 連続するキーの範囲を各パーティションに割り当てる
百科事典の各巻のように。
各範囲の境界がわかっていれば、
どのパーティションがどのキーを含んでいるか簡単に判断できる。
どのパーティションがどのノードに割りあたっているかわかっていれば、
適切なノードに直接リクエストできる。

キーの範囲は、均等になされる必要はない。
なぜならあ、
あなたのデータが均等に分散されているとは限らないからだ。
たとえば百科事典。
第一巻は A と B。
第十二巻は T, U, V, X, Y, and Z だったりする。
一巻につきアルファベット 2 つずつだと、巻によってサイズが大きく異なることになる。

データを均等に分散させるには、パーティションの境界はデータに適応させる必要がある。

パーティションの境界は、アドミンが手動で選んだり、
データベースが自動で選んだりする。
// 詳細は“Rebalancing Partitions”で改めて論じる。

こうしたパーティショニングストラテジーは、下記で使われている
Bigtable, its open source equivalent HBase , RethinkDB, and MongoDB before version 2.4.

各パーティションでは、キーをソートされた状態で維持できる。
結果、
キーの範囲スキャンがイージー。
そして、キーを複数の関連レコードを一度のクエリで取得するための、a concatenated index として扱うことができる。
(see “Multi-column indexes” on page 87).

たとえば、
ネットワークセンサーから取得したデータを格納するアプリを考えてみる。
キーは計測時のタイムスタンプ
(year-month-day-hour-minute-second).
Range scans によって、ある月の全ての読み込みが取得できる。

▼ key range partitioning の欠点
一部のアクセスパターンが ホットスポットを生む。

キーがタイムスタンプだとする。
時間範囲に応じたパーティションは、一日一パーティションだったりする。
センサーからDBに書き込むタイミングは、計測がなされたタイミングだとする。
そうすると、全ての書き込みは、当日に相当するパーティション一つになる。
で、他の日のパーティションはアイドル状態となる。

これを回避する方法。
the first element of the key に、タイムスタンプ以外のキーを使う必要がある。
たとえば、
タイムスタンプのプリフィックスとして、センサーの名前を使う。
すると、
パーティショニングはまずセンサー名によってなされ、次に時間によってなされる。

同じ時間に複数のセンサーがアクティブだとして、
書き込み負荷は複数のパーティションをまたがって、より均等になされるようになる。
複数のセンサーの同じタイムレンジの値を取得したいときは、
各センサー名の該当範囲に対してクエリを行うことになる。

■■■■■■■■■■■■■■■■■■■■■■■■■■ Partitioning by Hash of Key
skew とホットスポットのリスク対応として、
many distributed datastores は、あるキーのパーティションを決めるのに、
ハッシュを使っている。

A good hash function は、skewd data を均等に分散させることができる。

String を扱う 32-bit のハッシュファンクションがあるとする。
そいつに新しいStringを渡すたびに、
0 and 2^32 − 1 の間のランダムに近い値が返される。
インプットの文字列がよく似ていたとしても、
ハッシュ値は均等にある数値の範囲に分散される。

パーティショニング目的であれば、
ハッシュ機能は暗号的に強固である必要はない。
Cassandra and MongoDB use MD5, and Voldemort uses the Fowler–Noll–Vo function.

多くのプログラミング言語は、組み込みハッシュファンクションを持っている。
// ハッシュテーブルに使われたりするやつ
しかし、それらがパーティショニングに適しているとは限らない。
Java’s Object.hashCode() and Ruby’s Object#hash
上記井、同じキーが異なるハッシュ値を返すかもしれない。

適切なハッシュファンクションが決まれば、
各パーティションにハッシュ値の範囲を割り当てることができる。
で、キーのハッシュ値に応じて、各キーがパーティションに格納されていく。

これで、キーはパーティションに均等に分散される。
パーティションの境界は均等に拡散することができる
or they can be chosen pseudorandomly
(in which case the technique is sometimes known as consistent hashing).

-------------------------------------------------
■■■■■■■■■■■■■■■■■■■■■■■■■■ Consistent Hashing
Consistent hashing
content delivery network (CDN) のような、across an internet-wide system of caches に対して、
負荷を均等に分散するのに使われる。

ランダムに選ばれたパーティションの境界を使い、
avoid the need for central control or distributed consensus.
ここでの consistent は、
replica consistency (see Chapter 5)
or ACID consistency (see Chapter 7)
に対しては何もなさず、
, but rather describes a particular approach to rebalancing.

“Rebalancing Partitions”で改めて触れるが、
Consistent hashing はDBには向いていない。
よって、実際にはめったに使われていない。
(the documentation of some databases still refers to consistent hashing,
but it is often inaccurate).
Because this is so confusing, it’s best to avoid the term consistent hashing and just call it hash partitioning instead.
-------------------------------------------------

▼ 残念なお知らせ
パーティショニングにキーのハッシュを使うと、
key-range partitioning の利点である、効率的な範囲クエリができなくなる。
隣り合っていたキーがパーティション全体に分散され、
ソートオーダーがロストする。

MongoDB では、
hash-based sharding mode を有効化すると、レンジクエリーは全てのパーティションに送らなければならなくなる。

Riak , Couchbase , or Voldemort では、
プライマリキーに対するレンジクエリはサポートされていない。

▼ Cassandra の複合プライマリキー
Cassandra は2つのパーティショニング戦略を折衷した方法をとっている。
Cassandra のテール部は複数の列によって構成された複合プライマリキーを宣言できる。

キーの最初の部分のみが、パーティションを決めるためにハッシュ化され、
他の列は a concatenated index として使われる。
そしてそれがCassandra の SSTable でソートするために使われる。

結果、
クエリは複合キーの最初のコラムの値の範囲を検索できない。
しかし、
最初のコラムの a fixed value を特定できれば、
キーの他のコラムに対して効率的なレンジスキャンが実行できる。

The concatenated index approach は、1対多 関係において、エレガントなデータモデルを可能にする。
たとえば、
ソーシャルメディアサイトでのケース。
一人のユーザーは複数の交信をアップデートするかもしれない。
もし、その更新のプライマリキーが下記で構成されるならば、
(user_id, update_timestamp)
特定のユーザーによってなされた、ある期間のすべての更新を、タイムスタンプでソートして、効率的に取得できる。

他のユーザーは、他のパーティションに格納されているかもしれない。
しかし、各ユーザーは、
更新がタイムスタンプでソートされた状態で、一つのパーティションに格納される。

■■■■■■■■■■■■■■■■■■■■■■■■■■ Skewed Workloads and Relieving Hot Spots
パーティションを決めるためにキーをハッシュすることで、ホットスポットを減らすことの助けになる。
だが
完全に消し去ることはできない。
極端な例だが、
全ての読み書きが同じキーの場合、
全てのリクエストは同じパーティションにルートされる。

どんなケースか。
ソーシャルメディアサイトで、何百万ものフォロワーがいるセレブリティが何らかのアクティビティを行った際に、
これは起こりうる。
結果、同じキーに大量の書き込みが集中する。
// キーはセレブリティのユーザーIDもしくは、人々のコメントアクションのID
同じIDがキーならば、ハッシュも全ておなじになる。

現状、ほとんどのデータシステムは上記のように極端に歪んだ負荷を自動で埋め合わせることができない。
よって、
歪みを減らることは、アプリケーション側の責任となる。

例えば、
あるキーがすごく hot だとわかっていたら、
そのキーの最初または最後に、ランダム文字列を追記すればいい。
二桁のランダム数値をつけるだけで、書き込みキーを、均等に100の異なるキーに変えることができる。
結果、
それらキーは異なるパーティションに分散されることになる。

しかし、書き込みを異なるキーに分ける結果、
あらゆるキーが、追加の処理をしなくてはならなくなる。
あらゆるキーが、
100 のハッシュに分散されたキーを読み込んで、単一のキーにする、という処理をする必要がある。
加えて、
追加の"bookkeeping"も必要になる。
ランダム値を追加するのは、少数のホットなキーたちにのみ効果があり、
他の殆どの書き込みスループットの小さいキーにとっては、不要なオーバーヘッドが追加されることになる。

だから// こういうことを避けるためには、というニュアンスか？
何らかの方法で、どのキーがスプリットされたのか追跡する必要がある。

いつかはデータシステムが自動的に歪みを埋め合わせる方法を提供するようになるかもしれないが、
現状は、
アプリ側でトレードオフを考える必要がある。
